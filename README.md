# ScrabbleGAN - Handwritten Text Generation
A complete `requirements.txt` file will be added soon.

## Steps for training the ScrabbleGAN model from scratch
1. Download the dataset
and keep them in the data `/data/` directory as shown below:
    ```bash
    ├── data
    |   ├── hindi
    |       └──Lexicon
    |           └──words.txt (the vocab.txt file of the hindi dataset, format of words.txt : only texts)
    |       └──words
    |           └──train/
    |           └──test/
    |           └──val/
    |           └──train.txt/ (format of data in txt file: img_path text)
    |           └──test.txt/
    |           └──val.txt/
    |       └──Here will come the generated char_map for the hindi by running the prepare_data.py follow the steps.
    |   └── prepare_data.py 
   ```
   
2. Modify the `/config.py` file to change dataset, model architecture , image height, etc. 
The default parameters indicate the ones used in the paper. (Already done by me)
3. From the `data` directory, run:
    ```bash
    python prepare_data.py
    ```
    This will process the char_map , word_data and num_chars and create a pickle file to be used for training. 

4. Start model training by running the below command from the main directory:
    ```bash
    python train.py
    ```
   This will start training the model. A sample generated image will be saved in the `output` directory
   after every epoch. Tensorboard logging has also been enabled.  
   And The model checkpoint will be saved in every 5 epochs.

## Steps for generating new images
```
from config import Config
import pickle as pkl
from generate_images import ImgGenerator
import matplotlib.pyplot as plt
import torch
import numpy as np

dataset='hindi'
config = Config
config.dataset = 'hindi'
config.lexicon_file = '/data/Lexique383.tsv' if dataset == 'RIMES' else '/content/drive/MyDrive/scrabble/ScrabbleGAN/data/Lexicon/words.txt'
config.num_chars = 74 if dataset == 'IAM' else 109

with open(f'/content/drive/MyDrive/scrabble/ScrabbleGAN/data/hindi_tr_data.pkl', 'rb') as f:
   char_map = pkl.load(f)
char_map=char_map['char_map']
generator = ImgGenerator(checkpt_path=f'/weights/model_checkpoint_epoch_10.pth.tar',
                         config=config, char_map=char_map) # remember to change the path of the model according to your needs
sentences = ["कि"]
for word_list in sentences:
    word_list = word_list.split(' ')
    generated_imgs, _, word_labels = generator.generate(word_list=word_list)
    sentence_img = []
    for label, img in zip(word_labels, generated_imgs):
        img = img[:, img.sum(0) < 31.5]
        sentence_img.append(img)
        sentence_img.append(np.ones((img.shape[0], 15)))
    sentence_img = np.hstack(sentence_img)
    plt.imshow(sentence_img, cmap='gray')
    plt.axis('off')
    plt.show()                         
```

## Steps to check FID score
Create the preprocessed data file as described in steps 1-3 of "Steps for training the ScrabbleGAN model from scratch".
Also, either download the model checkpoints for [English (IAM)](https://drive.google.com/file/d/11w1p8RVLml9cidMrkQpdo648pNPdQFxZ/view?usp=sharing)
or [French (RIMES)](https://drive.google.com/file/d/16oasVsBExwHhCmYDSR1uhV10NiWYZ-OY/view?usp=sharing), or
train your own model and save the checkpoints. To check the FID score, run:
    ```bash 
    python calculate_metrics.py -c 'path_to_checkpoint_file'
    ```

## Steps for training HTR models
One of the motivation in the paper was to boost the HTR performance using synthetic data generated by ScrabbleGAN.
The code for HTR training has not been provided in this repository for consistency with the author's
approach of using this [code](https://github.com/clovaai/deep-text-recognition-benchmark) for HTR training.
You can follow the below steps for HTR training:
1. Create your own models or download all the files listed in "Steps for generating new images". Also, 
create the preprocessed data file as described in steps 1-3 of "Steps for training the ScrabbleGAN model from scratch".
2. If required, change `dataset`, `partition`, `data_file`, `lexicon_file` in `config.py`
3. To create LMDB data files required for HTR training, run:
    ```bash 
    python create_lmdb_dataset.py -c 'path_to_checkpoint_file' -m 'path_to_character_mapping_file'
    ```
   to create lmdb dataset without any synthetic images, or
    ```bash 
    python create_lmdb_dataset.py -c 'path_to_checkpoint_file' -m 'path_to_character_mapping_file' -n 100000
    ```
    to add generated images to the original dataset.
4. Train the HTR model as described [here](https://github.com/clovaai/deep-text-recognition-benchmark#when-you-need-to-train-on-your-own-dataset-or-non-latin-language-datasets)

